# coding: utf-8

"""
    Pulze.ai API

    At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.  We are a VC Funded, early stage startup based in San Francisco.

    The version of the OpenAPI document: 0.1.0
    Generated by: https://konfigthis.com
"""

from datetime import datetime, date
import typing
from enum import Enum
from typing_extensions import TypedDict, Literal, TYPE_CHECKING

from pulze_ai_python_sdk.type.model_parts import ModelParts
from pulze_ai_python_sdk.type.pulze_engine_response_metadata_labels import PulzeEngineResponseMetadataLabels
from pulze_ai_python_sdk.type.pulze_engine_tokens import PulzeEngineTokens
from pulze_ai_python_sdk.type.ranked_scoring_models import RankedScoringModels

class RequiredPulzeEngineResponseMetadata(TypedDict):
    pass

class OptionalPulzeEngineResponseMetadata(TypedDict, total=False):
    # The ID of the app this request belongs to
    app_id: str

    # Category assigned to this request (Science, Health, Games...)
    category: str

    # Price difference -- compared with GPT-4
    cost_savings: PulzeEngineTokens

    # Cost (in $) of the request
    costs: PulzeEngineTokens

    # If an error occurs, it will be stored here
    error: str

    # Extra data
    extra: typing.Dict[str, typing.Union[bool, date, datetime, dict, float, int, list, str, None]]

    labels: PulzeEngineResponseMetadataLabels

    # The time it took for the Provider to return a response
    latency: typing.Union[int, float]

    # Maximum number of tokens that can be used in the request+response.Leave empty to make it automatic, and set to `-1` to use the maximum context size (model-dependent)
    max_tokens: int

    # The model used in the request
    model: ModelParts

    # The number of retries needed to get the answer. `null` or `0` means no retries were required
    retries: int

    # The score for the currently used LLM
    score: typing.Union[int, float]

    # A ranking of the best models for a given request
    scores: RankedScoringModels

    # Status code of the response
    status_code: int

    # Temperature used for the request
    temperature: typing.Union[typing.Union[int, float], int]

class PulzeEngineResponseMetadata(RequiredPulzeEngineResponseMetadata, OptionalPulzeEngineResponseMetadata):
    pass
