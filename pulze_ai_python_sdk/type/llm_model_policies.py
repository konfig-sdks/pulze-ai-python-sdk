# coding: utf-8

"""
    Pulze.ai API

    At Pulze it's our mission to supercharge today's workforce with AI to maximize the world's prosperity. We are doing so by enabling companies of any size to securely leverage Large Language Models (LLM) and easily build AI features into their apps. Our enterprise platform has access to all best in class LLMs and can route user requests to the most relevant model to get the highest quality response at the best price thanks to our smart meta model. End users can leverage pre-built applications, such as our Marketing AI product, or build custom apps on top of the Pulze Platform.  We are a VC Funded, early stage startup based in San Francisco.

    The version of the OpenAPI document: 0.1.0
    Generated by: https://konfigthis.com
"""

from datetime import datetime, date
import typing
from enum import Enum
from typing_extensions import TypedDict, Literal, TYPE_CHECKING


class RequiredLLMModelPolicies(TypedDict):
    pass

class OptionalLLMModelPolicies(TypedDict, total=False):
    # The maximum cost allowed for a request. Only works with compounded requests that require multiple LLM calls. If the value is reached, it will exit with an exception.
    max_cost: typing.Union[int, float]

    # If an LLM call fails, how many times should Pulze _retry the call to the same LLM_? There will be a maximum of N+1 calls (original + N retries)
    max_same_model_retries: int

    # If an LLM call fails, _how many other models_ should Pulze try, chosen by quality descending? It will be a maximum of N+1 models (original + N other models)
    max_switch_model_retries: int

    # Optimize the internal / intermediate LLM requests, for a big gain in speed and cost savings, at the cost of a potential, and very slight, penalty on quality. The final request (\"SYNTHESIZE\") is always performed using your original settings.
    optimize_internal_requests: int

    #          The level of privacy for a given request         0 = (UNSUPPORTED -- public logs)         1 = Log request, response and all of its metadata (Normal mode)         2 = Do not log neither the request prompt nor the response text. Logs are still visible, and all of the request metadata accessible. Retrievable as a log. (TBD)         3 = Do not log at all. Internally, a minimal representation may be stored for billing: model name, tokens used, which app it belongs to, and timestamp. Not retrievable as a log. (TBD)         
    privacy_level: int

    # Prompt ID that we will use for requests
    prompt_id: str

class LLMModelPolicies(RequiredLLMModelPolicies, OptionalLLMModelPolicies):
    pass
